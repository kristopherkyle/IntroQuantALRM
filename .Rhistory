geom_boxplot(mapping = aes(x = Prompt, y = Nwords))
ggplot(data = mydata) +
geom_boxplot(aes(x = Prompt, y = Nwords))
ggplot(data = mydata) +
geom_boxplot(aes(x = Prompt, y = Nwords, color = Prompt))
install.packages("car")
library(car)
leveneTest(Nwords ~ Prompt, mydata) #the syntax here is variable, grouping variable, dataframe
#install.packages("car")
library(car)
leveneTest(Nwords ~ Prompt, mydata) #the syntax here is variable, grouping variable, dataframe
ggplot(data = mydata) +
geom_boxplot(mapping = aes(x = Prompt, y = Nwords,color = Prompt))
ggplot(data = mydata) +
geom_violin(mapping = aes(x = Prompt, y = Nwords,color = Prompt)) +
geom_boxplot(mapping = aes(x = Prompt, y = Nwords), width = .2)
t.test(Nwords~Prompt, alternative = 'two.sided', conf.level = .95, var.equal = TRUE, data = mydata)
#install.packages("psych") #install this package if needed
library(psych)
cohen.d(mydata,"Prompt") #this will generate results for all variables in the data
wilcox.test(Nwords~Prompt,data = mydata)
install.packages("rcompanion")
library(rcompanion) #don't forget to install if you haven't already
wilcoxonR(mydata$Nwords,mydata$Prompt)
knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)
mydata <- read.csv("data/anova_sample.csv", header = TRUE)
summary(mydata)
library(ggplot2)
ggplot(mydata, aes(x=Meaningfulness_AW, color = Proficiency, fill=Proficiency)) +
geom_density(alpha = 0.4) #note that "alpha" sets the level of color transparency
#load dplyr package, which helps us manipulate datasets:
library(dplyr)
#create a new dataframe that includes only Beginner:
beginner.ds <- mydata %>% filter(Proficiency == "Beginner")
#create a new dataframe that includes only Int:
intermediate.ds <- mydata %>% filter(Proficiency == "Int")
#create a new dataframe that includes only High:
high.ds <- mydata %>% filter(Proficiency == "High")
#Test normality for Meaningfulness_AW in Beginner essays
shapiro.test(beginner.ds$Meaningfulness_AW) #p = 0.2336
#Test normality for Meaningfulness_AW in Int essays
shapiro.test(intermediate.ds$Meaningfulness_AW) #p = 0.0112
#Test normality for Meaningfulness_AW in High essays
shapiro.test(high.ds$Meaningfulness_AW) #p = 0.1591
ggplot(data = mydata) +
geom_boxplot(mapping = aes(x = Proficiency, y = Meaningfulness_AW))
ggplot(data = mydata) +
geom_boxplot(aes(x = reorder(Proficiency, Meaningfulness_AW, FUN = median), y = Meaningfulness_AW, color = Proficiency)) +
xlab("Proficiency")
library(car)
leveneTest(Meaningfulness_AW ~ Proficiency, mydata) #the syntax here is variable ~ grouping variable, dataframe
library(lsr)
install.packages("lsr") #don't forget to install this package if you haven't already done so
library(lsr)
anova_model <- aov(Meaningfulness_AW~Proficiency, data = mydata)
summary(anova_model)
etaSquared(anova_model, type = 3)
TukeyHSD(anova_model)
#create new df with only Beginner and Int
mydata_beg_int <- mydata %>%
filter(Proficiency == "Beginner" | Proficiency == "Int") %>%
select(Proficiency, Meaningfulness_AW)
#summary(mydata_effs_BI)
library(psych)
cohen.d(mydata_beg_int,"Proficiency")# (-).54 [medium effect]
#create new df with only Beginner and Int
mydata_beg_int <- mydata %>%
filter(Proficiency == "Beginner" | Proficiency == "Int") %>%
select(Proficiency, Meaningfulness_AW)
#summary(mydata_effs_BI)
library(psych)
cohen.d(mydata_beg_int,"Proficiency")# (-).54 [medium effect]
#create new df with only Beginner and High
mydata_beg_high <- mydata %>%
filter(Proficiency == "Beginner" | Proficiency == "High") %>%
select(Proficiency, Meaningfulness_AW)
#summary(mydata_effs_BH)
#create new df with only Int and High
mydata_int_high <- mydata %>%
filter(Proficiency == "Int" | Proficiency == "High") %>%
select(Proficiency, Meaningfulness_AW)
#summary(mydata_effs_IH)
cohen.d(mydata_beg_high,"Proficiency")# (-)1.2 [large effect]
cohen.d(mydata_int_high,"Proficiency")# .56 [medium effect]
welch_model <- oneway.test(Meaningfulness_AW~Proficiency, data = mydata,var.equal = FALSE)
print(welch_model)
library(userfriendlyscience)
install.packages("userfriendlyscience")
library(userfriendlyscience)
install.packages("userfriendlyscience")
library(userfriendlyscience)
install.packages('userfriendlyscience')
library(userfriendlyscience)
install.packages('userfriendlyscience')
#library(userfriendlyscience)
#posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
library(userfriendlyscience)
install.packages('userfriendlyscience')
#library(userfriendlyscience)
#posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
require(userfriendlyscience)
install.packages('userfriendlyscience',
contriburl='https://userfriendlyscience.com/src/contrib',
type='source', dependencies=TRUE);
#install.packages('userfriendlyscience')
library(userfriendlyscience)
install.packages('games_howell_test')
#library(userfriendlyscience)
#posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
install.packages('games_howell_test')
install.packages("games_howell")
install.packages("userfriendlyscience")
install.packages("usf")
devtools::install_github("matherion/userfriendlyscience", dependencies=TRUE)
install.packages("devtools")
devtools::install_github("matherion/userfriendlyscience", dependencies=TRUE)
#this isn't working anymore...  need to find alternative
#install.packages('games_howell_test')
library(userfriendlyscience)
posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
#this isn't working anymore...  need to find alternative
#install.packages('games_howell_test')
library(userfriendlyscience)
usf::posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
#this isn't working anymore...  need to find alternative
#install.packages('games_howell_test')
library(userfriendlyscience)
userfriendlyscience::posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
#install.packages('userfriendlyscience') #install if you don't have it yet
#Note that with some versions of R, you may have to first install devtools, then follow the instructions below:
#install.packages("devtools")
#devtools::install_github("matherion/userfriendlyscience", dependencies=TRUE)
library(userfriendlyscience)
posthocTGH(mydata$Proficiency, y = mydata$Meaningfulness_AW)
kruskal.test(Meaningfulness_AW~Proficiency, data = mydata)
library(FSA)
dunnTest(Meaningfulness_AW~Proficiency, data = mydata, method = "bh")
#create new df with only Beginner and Int
mydata_beg_int <- mydata %>%
filter(Proficiency == "Beginner" | Proficiency == "Int") %>%
select(Proficiency, Meaningfulness_AW)
#summary(mydata_effs_BI)
library(psych)
cohen.d(mydata_beg_int,"Proficiency") # (-).54 [medium effect]
library(psych)
describe(mydata$Nwords)
library(psych)
describe(mydata)
mydata <- read.csv("data/distribution_sample.csv", header = TRUE) #this presumes that we have a folder in our working directory named "data" and that we have a file named "distribution sample" in that folder.
summary(mydata)
library(psych)
describe.by(mydata)
library(psych)
describe.by(mydata,promptA)
library(psych)
describe.by(mydata)
library(psych)
describe.by(mydata,group = "Prompt")
library(psych)
describe.by(mydata,group = "PromptA")
library(psych)
describe.by(mydata,group = "Prompt")
library(psych)
describeBy(mydata,group = "Prompt")
library(psych)
library(dplyr)
describe(mydata) #get descriptive statistics
#load dplyr package, which helps us manipulate datasets:
library(dplyr)
#create a new dataframe that includes only responses to Prompt A:
promptA <- mydata %>% filter(Prompt == "A")
describe(promptA)
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata %>% filter(Prompt == "B")
describe(promptB)
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata %>% select(filter(Prompt == "B"),c(,"Nwords"))
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata %>% dplyr::select(filter(Prompt == "B"),c(,"Nwords"))
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata dplyr::%>% dplyr::select(filter(Prompt == "B"),c(,"Nwords"))
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata %>% dplyr::select(dplyr::filter(Prompt == "B"),c(,"Nwords"))
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata %>% dplyr::filter(Prompt == "B")
describe(promptB)
##create a new dataframe that includes only responses to Prompt b:
promptB <- mydata %>% filter(Prompt == "B")
describe(promptB)
library(ggplot2)
ggplot(mydata, aes(x=Nwords, color = Prompt)) +
geom_density() +
facet_wrap(~Prompt)
ggplot(mydata, aes(x=Nwords, color = Prompt, fill=Prompt)) +
geom_density(alpha = 0.4)
ggplot(data = mydata) +
geom_boxplot(aes(x = Prompt, y = Nwords, color = Prompt))
t.test(Nwords~Prompt, alternative = 'two.sided', conf.level = .95, var.equal = TRUE, data = mydata)
ggplot(data = mydata) +
geom_boxplot(mapping = aes(x = Prompt, y = Nwords,color = Prompt))
ggplot(data = mydata) +
geom_violin(mapping = aes(x = Prompt, y = Nwords,color = Prompt)) +
geom_boxplot(mapping = aes(x = Prompt, y = Nwords), width = .2)
#install.packages("psych") #install this package if needed
library(psych)
cohen.d(mydata,"Prompt") #this will generate results for all variables in the data
ggplot(data = mydata) +
geom_boxplot(mapping = aes(x = Prompt, y = Nwords,color = Prompt))
ggplot(data = mydata) +
geom_boxplot(mapping = aes(x = Prompt, y = Nwords,color = Prompt))
knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)
library(psych)
library(dplyr)
mfl <- mydata %>% dplyr::select(Meaningfulness_AW)
describe(mfl) #get descriptive statistics
mydata <- read.csv("data/anova_sample.csv", header = TRUE)
summary(mydata)
library(psych)
library(dplyr)
mfl <- mydata %>% dplyr::select(Meaningfulness_AW)
describe(mfl) #get descriptive statistics
library(psych)
library(dplyr)
mfl <- mydata %>% dplyr::select(Proficiency,Meaningfulness_AW)
describe(mfl) #get descriptive statistics
mfl.beg <- mfl %>% dplyr:: filter(Proficiency == "Beginner")
describe(mfl) #get descriptive statistics
mfl.int <- mfl %>% dplyr:: filter(Proficiency == "Int")
describe(mfl.int) #get descriptive statistics
mfl.high <- mfl %>% dplyr:: filter(Proficiency == "High")
describe(mfl.high) #get descriptive statistics
mfl.beg <- mfl %>% dplyr:: filter(Proficiency == "Beginner")
describe(mflmfl.beg) #get descriptive statistics
mfl.beg <- mfl %>% dplyr:: filter(Proficiency == "Beginner")
describe(mfl.beg) #get descriptive statistics
ggplot(data = mydata) +
geom_boxplot(aes(x = reorder(Proficiency, Meaningfulness_AW, FUN = median), y = Meaningfulness_AW, color = Proficiency)) +
xlab("Proficiency")
library(ggplot2)
ggplot(data = mydata) +
geom_boxplot(aes(x = reorder(Proficiency, Meaningfulness_AW, FUN = median), y = Meaningfulness_AW, color = Proficiency)) +
xlab("Proficiency")
#install.packages("lsr") #don't forget to install this package if you haven't already done so
library(lsr)
anova_model <- aov(Meaningfulness_AW~Proficiency, data = mydata)
summary(anova_model)
etaSquared(anova_model, type = 3)
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
ggplot(cor_data, aes(x = nwords, y=frequency_CW )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
library(ggplot2) #load ggplot2
cor_data <- read.csv("data/correlation_sample.csv", header = TRUE) #read the spreadsheet "correlation_sample.csv" into r as a dataframe
summary(cor_data) #get descriptive statistics for the dataset
ggplot(cor_data, aes(x = nwords, y=frequency_CW )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
ggplot(cor_data, aes(x = nwords, y=frequency_CW )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
ggplot(cor_data, aes(x = nwords, y=frequency_CW )) +
stat_density_2d(aes(fill = ..level..), geom = "polygon")
library(dplyr)
library(QuantPsyc)
cor_data.small <- cor_data %>% dplyr::select(nwords,frequency_CW)
mult.norm(cor_data.small)$mult.test
cor.test(cor_data$nwords,cor_data$frequency_CW)
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
library(ggplot2) #load ggplot2
cor_data <- read.csv("data/correlation_sample.csv", header = TRUE) #read the spreadsheet "correlation_sample.csv" into r as a dataframe
summary(cor_data) #get descriptive statistics for the dataset
ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
#define model1 as a regression model using frequency_CW to predict nwords
model1 <- lm(nwords ~ frequency_CW, data = cor_data)
#define model1 as a regression model using frequency_CW to predict nwords
summary(model1)
ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
ggplot(cor_data, aes(x = frequency_CW, y=nwords )) +
stat_density_2d(aes(fill = ..level..), geom = "polygon")
library(car)
qqPlot(model1)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
mydata <- read.csv("data/RM_sample.csv", header = TRUE)
#First, we create a new variable that is the categorical version of Time
mydata$FTime <- factor(mydata$Time)
summary(mydata)
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Time)) +
geom_boxplot()
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
geom_line(aes(color=Participant)) +
geom_point(aes(color=Participant))
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
geom_line(aes(color=Participant)) +
geom_point(aes(color=Participant)) +
facet_wrap(~Participant)
library(lmerTest)
int_model1 <- lmer(MLT~FTime + (1 | Participant), data=mydata)
summary(int_model1)
linearity<-plot(resid(int_model1),#extract the residuals
mydata$MLT) #specify original y variable
library(lattice)
qqmath(int_model1)
library(MuMIn)
r.squaredGLMM(int_model1)
library(emmeans)
int_model1.emm <- emmeans(int_model1, specs = pairwise~FTime)
summary(int_model1.emm)
int_model2 <- lmer(MLT~Time + (1 | Participant), data=mydata)
summary(int_model2) #get model summary
ranef(int_model2) #get slope adjustments per participant
r.squaredGLMM(int_model2) #get effect sizes
library(dplyr)
#add predicted values to dataframe
mydata <- mydata %>% mutate(int_model2_pred = predict(int_model2,re.form = ~ (1 | Participant)))
ggplot(mydata, aes(y=MLT, x=Time)) +
facet_wrap(~ Participant) +
geom_point(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(y=int_model2_pred), linetype=2) +
scale_y_continuous()
int_slope_model <- lmer(MLT~Time + (1 + Time | Participant), data=mydata)
summary(int_slope_model) #get model summary
ranef(int_slope_model)
r.squaredGLMM(int_slope_model)
library(dplyr)
mydata <- mydata %>% mutate(int_slope_model_pred = predict(int_slope_model,re.form = ~ (1 | Participant)))
ggplot(mydata, aes(y=MLT, x=Time)) +
facet_wrap(~ Participant) +
geom_point(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(y=int_slope_model_pred), linetype=2) +
scale_y_continuous()
anova(int_model2,int_slope_model)
library(psych)
describe(cor_data) #get descriptive statistics
cor.test(cor_data$nwords,cor_data$frequency_CW)
#define model1 as a regression model using frequency_CW to predict nwords
model1 <- lm(nwords ~ frequency_CW, data = cor_data)
#define model1 as a regression model using frequency_CW to predict nwords
summary(model1)
ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
#geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
(2.2 * -151.42) + 729.93
(2.8*-151.42) + 729.93
ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
#geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Time)) +
geom_boxplot()
mydata <- read.csv("data/RM_sample.csv", header = TRUE)
#First, we create a new variable that is the categorical version of Time
mydata$FTime <- factor(mydata$Time)
summary(mydata)
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Time)) +
geom_boxplot()
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
geom_line(aes(color=Participant)) +
geom_point(aes(color=Participant))
library(lmerTest)
int_model1 <- lmer(MLT~FTime + (1 | Participant), data=mydata)
summary(int_model1)
ranef(int_model1)
library(lattice)
qqmath(int_model1)
#install.packages("lattice") #if not already installed
library(lattice)
qqmath(int_model1)
library(MuMIn)
r.squaredGLMM(int_model1)
#install.packages("emmeans") #if not already installed
library(emmeans)
int_model1.emm <- emmeans(int_model1, specs = pairwise~FTime)
summary(int_model1.emm)
int_model2 <- lmer(MLT~Time + (1 | Participant), data=mydata)
summary(int_model2) #get model summary
ranef(int_model2) #get slope adjustments per participant
r.squaredGLMM(int_model2) #get effect sizes
library(dplyr)
#add predicted values to dataframe
mydata <- mydata %>% mutate(int_model2_pred = predict(int_model2,re.form = ~ (1 | Participant)))
ggplot(mydata, aes(y=MLT, x=Time)) +
facet_wrap(~ Participant) +
geom_point(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(y=int_model2_pred), linetype=2) +
scale_y_continuous()
int_slope_model <- lmer(MLT~Time + (1 + Time | Participant), data=mydata)
summary(int_slope_model) #get model summary
ranef(int_slope_model)
r.squaredGLMM(int_slope_model)
library(dplyr)
mydata <- mydata %>% mutate(int_slope_model_pred = predict(int_slope_model,re.form = ~ (1 | Participant)))
ggplot(mydata, aes(y=MLT, x=Time)) +
facet_wrap(~ Participant) +
geom_point(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(color=Participant), show.legend = FALSE) +
geom_line(aes(y=int_slope_model_pred), linetype=2) +
scale_y_continuous()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
library(ggplot2) #load ggplot2
cor_data <- read.csv("data/correlation_sample.csv", header = TRUE) #read the spreadsheet "correlation_sample.csv" into r as a dataframe
summary(cor_data) #get descriptive statistics for the dataset
ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the entire dataset
scale_color_viridis(discrete = TRUE, option = "D") +  # This applies the viridis color scale
theme_minimal()  # Optional: Applies a minimalistic theme
library(ggplot2) #load ggplot2
library(viridis) #color-friendly palettes
cor_data <- read.csv("data/correlation_sample.csv", header = TRUE) #read the spreadsheet "correlation_sample.csv" into r as a dataframe
summary(cor_data) #get descriptive statistics for the dataset
ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the entire dataset
scale_color_viridis(discrete = TRUE, option = "D") +  # This applies the viridis color scale
theme_minimal()  # Optional: Applies a minimalistic theme
g1 <- ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the entire dataset
scale_color_viridis(discrete = TRUE) +
theme_minimal()  # Optional: Applies a minimalistic theme
g1 <- ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the entire dataset
scale_color_viridis(discrete = TRUE) +
theme_minimal()  # Optional: Applies a minimalistic theme
print(g1)
ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the enture dataset
scale_color_viridis(discrete = TRUE) +
theme_minimal()
g2 <- ggplot(cor_data, aes(x = frequency_CW, y= nwords )) +
geom_point() +
geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the enture dataset
scale_color_viridis(discrete = TRUE) +
theme_minimal()
print(g2)
ggplot(cor_data, aes(x = frequency_CW, y=nwords )) +
stat_density_2d(aes(fill = ..level..), geom = "polygon")
library(dplyr)
library(QuantPsyc)
cor_data.small <- cor_data %>% dplyr::select(nwords,frequency_CW)
mult.norm(cor_data.small)$mult.test
cor.test(cor_data$nwords,cor_data$frequency_CW)
ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
#geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") #this is a line of best fit based on the enture dataset
cor.test(cor_data$nwords,cor_data$frequency_CW,method = ("spearman"))
cor.test(cor_data$nwords,cor_data$frequency_CW,method = ("kendall"))
ggplot(cor_data, aes(x = frequency_CW, y=nwords )) +
stat_density_2d(aes(fill = ..level..), geom = "polygon")+
scale_fill_viridis() +
theme_minimal()
g2 <- ggplot(cor_data, aes(x = frequency_CW, y=nwords )) +
stat_density_2d(aes(fill = ..level..), geom = "polygon")+
scale_fill_viridis() +
theme_minimal()
print(g2)
cat('<img src="plot1.png" alt="Scatter plot showing black points and two smooth lines running through the data points: one in blue, which shows a line of best fit, and the other in red, which shows a loess regression. The red line meant to approximate the blue line. The gray shaded area around each line indicates the confidence interval for that regression. The distribution of points appears random without obvious clusters.">')
g3 <- ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
#geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the entire dataset
scale_color_viridis(discrete = TRUE) +
theme_minimal()
print(g3)
g3 <- ggplot(cor_data, aes(x = frequency_CW , y=nwords )) +
geom_point() +
#geom_smooth(method = "loess",color = "red") + #this is a line of best fit based on a moving average
geom_smooth(method = "lm") + #this is a line of best fit based on the entire dataset
scale_color_viridis(discrete = TRUE) +
theme_minimal()
#print(g3)
