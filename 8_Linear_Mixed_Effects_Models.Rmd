---
title: "Linear Mixed-Effects Models (LME) for Time Series Data "
author: "Kristopher Kyle"
date: "updated 2022-02-08"
output:
  html_document:
    toc: true
    toc_float: true
---
[Back to Homepage](https://kristopherkyle.github.io/IntroQuantALRM/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
# Linear Mixed-Effects (LME) Models
Linear mixed effects models are mathematically and conceptually related to a linear regression (and accordingly to an ANOVA). The biggest difference between and LME and a linear regression is that an LME can adjust the line of best fit based on trajectories of particular individuals (or groups). In this tutorial, we will use linear mixed-effects models to examine the relationship between time spent learning English as an L2 and writing development (measured via an index of syntactic complexity). 

We will first treat Time as a categorical variable to see the degree to which changes in syntactic complexity occur between data collection points. This analysis is related to a repeated measures ANOVA both conceptually and mathematically. However, an LME is arguably more precise than an ANOVA because it takes into account the individual trajectories of each participant.

We will then treat Time as a continuous variable to see the degree to which changes in syntactic complexity are linear (i.e., follow a reasonably consistent trajectory).

## Description of sample data

This data comprises L2 English essays written over a two year period by nine middle-school aged Dutch children studying at an English/Dutch bilingual school in the Netherlands. Essays were collected three times a year (every four months) over two academic years. Included in the dataset are holistic scores for each essay ("Score") and mean length of T-unit (MLT) values. In this tutorial, we will explore the relationship between MLT and time spent studying English, with the alternative hypothesis that MLT scores will increase as a function of time. For further reference, see Kyle, Crossley, & Verspoor (2021) and Kyle (2016). In other words, we will be attempting to determine whether (and the degree to which) Dutch EFL middle school students write more words per T-unit (a T-unit is an independent clause and all connected depedent clauses) as a function of the time they spend studying English.

```{r}
mydata <- read.csv("data/RM_sample.csv", header = TRUE)
#First, we create a new variable that is the categorical version of Time
mydata$FTime <- factor(mydata$Time)
summary(mydata)
```

## Looking at trends in the data through visualization

First, we can look at the means at each time point. Overall, there appears to be a positive trend (i.e., more words per T-unit at later essay collection points), but this is not perfectly linear.

```{r}
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Time)) +
  geom_boxplot()
```

Then, we can get a clearer view by looking at individual trajectories. In the plot below, each line represents a different participant.

```{r}
library(ggplot2)
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
  geom_line(aes(color=Participant)) +
  geom_point(aes(color=Participant))
```
Sometimes, we get a clearer view of individual trajectories when we use facet wrap. As we can see, participants seem to be generally following an upward trend, but there are also non-linear trends (see EFL_6 and EFL 7 at week 3!).

```{r}
ggplot(data = mydata, aes(x = FTime, y = MLT, group = Participant)) +
  geom_line(aes(color=Participant)) +
  geom_point(aes(color=Participant)) +
  facet_wrap(~Participant)
```

## Conducting an LME
Linear mixed effects models allow us to account for both fixed effects (these are the variables we are most interested in, such as time spent studying English) and random effects, which are variables that may affect our results, but are not the main variables of interest. 

In this tutorial, we are primarily interested in time spent studying English (Time and/or FTime). However, many researchers have highlighted the idea that individuals take varying paths on their way to proficiency, so individual variation may affect our results. This is our random effect, because we want to account for individual variation, but it isn't our primary variable of interest.

### Assumptions
Linear mixed effects (lme) models are relatively new (particularly when compared to statistics such as t-tests). They are also particularly flexible. Due to these two issues, there is not a fully accepted consensus as to the assumptions for running an lme. However we will check the following two assumptions, which are the same for linear regression models:

- the relationship between the residuals (the model error for each data point) and the variable being predicted should be linear
- the residuals should be normally distributed

Note that to check residuals we will have to run our models first. So, we will (a bit paradoxically) check our assumptions AFTER running our models.

### Random intercepts

As noted above, our random effects are our participants (i.e., our main interest is not in inter-participant variation, but we still need to control for it). In the first model we will run, we will presume that all of our participants will follow similar trajectories, but may have different starting points (intercepts). In other words, the model will presume that participants will increase their MLT scores at approximately the same rate, but may have be at different writing proficiencies (at least with regard to MLT) at the beginning of the study. Also, first, we will run the analysis with time as a categorical variable (FTime), like we would with a repeated measures ANOVA. Then, we will run it as a continuous variable to show the difference. 

The most "appropriate" choice (treating Time as a factor or a continuos variable) will depend on our goals. If we want to see WHERE changes occurred (e.g., across particular time points), then we will want to treat time as a categorical variable. If we want to see the strength of the relationship (i.e., how strongly [and linearly] related Time and MLT scores are), then we will want to treat Time as a continous variable.

```{r}
library(lmerTest)
int_model1 <- lmer(MLT~FTime + (1 | Participant), data=mydata)
summary(int_model1)
```

Note that the "intercept" here presumes that FTime = 1, and all of the data points are in reference to this. So, from FTime 1 to FTime 2, there is a .6667 increase in words per T-unit (which is not significant). Additionally, from FTime 1 to FTime 3, there is a 1.7767 increase in words per T-unit (which is not significant). We don't get a significant change until we get to FTime 5. 

The Random Effects section above indicates that there is indeed variation across participants at each time point (the standard deviation is 1.102 words per T-unit).

We can also see the degree to which our estimates need to change for each participant (i.e., we can see our individualized random effects).

```{r}
ranef(int_model1)
```

Again, this is a random intercepts model, so the only changes we will see is in the intercept (i.e., starting point). All other estimates will be in relation to these revised intercepts. So, for example, at Time 1, EFL_1's estimated MLT score will be 9.6968 - 0.3623567 = 9.334443, while EFL_2's estimated MLT score will be 9.6968 + 1.2929820 = 10.98978. But remember, these are estimates created under the assumption that all participants will have the same slope (or rate of change), and we can see from looking at our plots that this estimate is decent for EFL_1, but rather poor for EFL_2.

#### Checking assumptions
Now that we have a model, we can check the assumption of linearity and normality.

First we will test for linearity (wherein the relationship between our actual MLT values and our predicted MLT values should roughly comprise a straight line as opposed to a curved one!). As we see below, the model appears to satisfy the assumption of linearity.


```{r}
linearity<-plot(resid(int_model1),#extract the residuals
                mydata$MLT) #specify original y variable
```

Now, we will check the assumption of normality (of residuals) using a qq plot. A perfectly normal distribution of residuals would be when all data points fall perfectly on the line. As we can see, we have some outliers, but the distribution is (arguably) roughly normal.

```{r}
#install.packages("lattice") #if not already installed
library(lattice)
qqmath(int_model1)
```

#### Model effect size
Now that we have checked (and at least arguably met) assumptions, we can proceed to computing effect sizes.

```{r}
#install.packages("MuMIn") #if not alread installed
library(MuMIn)
r.squaredGLMM(int_model1)
```

The "MuMIN" package gives us two R^2 values. The first, R2m (marginal r-squared), is the effect size without considering the random effects (i.e., where the model is built without regard to participant differences). The second, R^2c (conditional r-squared) is the effect size when the random effects are taken into account. Accordingly, the R^2c values are aways higher than the R^2m values (unless there are little/no random effects). 

The R^2m value here indicate that FTime accounts for 20.4% variance in MLT scores overall. The R^2c value indicates that participant variation + FTime together account for 35.5% of the variance in MLT scores.

#### Getting Pairwise Comparisons

Below, we use the package "emmeans" to get pairwise comparisons from our model (in case we want them). The emmeans packages is very flexible, so it is probably worthwhile to read the documentation on the package. As we can see below (look at the comparisons starting below the "$contrasts" line), the only significant pairwise difference (assuming an alpha level of .05) is between Time 1 and Time 5.

```{r}
#install.packages("emmeans") #if not already installed
library(emmeans)
int_model1.emm <- emmeans(int_model1, specs = pairwise~FTime)
summary(int_model1.emm)
```


### Time as a Continuous Variable

We can also run our analysis with Time as a continuous variable (the distance between each Time value is roughly 4-months). In this case, we will be testing whether the line of best fit is a reasonable approximation of our data. Again, at this point, we are still working with a random intercepts model.

```{r}
int_model2 <- lmer(MLT~Time + (1 | Participant), data=mydata)
summary(int_model2) #get model summary
ranef(int_model2) #get slope adjustments per participant
r.squaredGLMM(int_model2) #get effect sizes
```

This indicates that Time is a significant predictor of MLT scores (p = .00027) with medium effects (R2^m = 0.191, R^2c = 0.355). Note below, we add the predicted scores for each participant to our dataframe using the "mutate" function of the package "dplyr". Then, we plot the predicted line (dashed) on top of the actual data. We see that each line has the same slope, but different intercepts.

```{r}
library(dplyr)
#add predicted values to dataframe
mydata <- mydata %>% mutate(int_model2_pred = predict(int_model2,re.form = ~ (1 | Participant)))

ggplot(mydata, aes(y=MLT, x=Time)) +
  facet_wrap(~ Participant) + 
  geom_point(aes(color=Participant), show.legend = FALSE) + 
  geom_line(aes(color=Participant), show.legend = FALSE) +
  geom_line(aes(y=int_model2_pred), linetype=2) +
  scale_y_continuous()
```

These results indicate that there is a significant, positive, medium relationship between Time and MLT. The results also indicate that there is a fairly high degree of variability across participants.

### Write up

A linear mixed effects model was run using Time to predict the mean length of T-unit (MLT) in the participant essays. The simpler, random intercepts model was indistinguishable from the random intercepts and slopes model, so it was retained. The results of the analysis indicated that was a significant positive relationship between Time and MLT in the participant essays (p < .001, R2m = .191, R2c = .355).

Table 1 comprises the estimates, standard errors, t values and p values for the fixed effect of Time. The results indicate that the participantsâ€™ wrote longer T-units over the two-year time period, at a rate of approximately .7 words per T-unit at progressive collection points. Approximately 19% of the variance in MLT scores was be explained by Time spent studying English, while approximately 16.5% of the the variance in MLT scores was explained by the random effects (variation across participants).Figure 1 includes plots of the MLT scores for each participant at each collection point with the regression lines (indicated by the dashed line) produced by the model.

Table 1.  
LME model predicting mean length of T-unit  

| Fixed Effects | Estimate | SE     | _t_     | _p_     |
| :------       | :---     | :----  | :---    | :---    |
| (Intercept)   | 9.064    | 0.784  | 11.560  | < 0.001 |
| Time          | 0.701    | 0.177  | 3.960   | < 0.001 |


```{r, echo = FALSE}
library(dplyr)
#add predicted values to dataframe
mydata <- mydata %>% mutate(int_model2_pred = predict(int_model2,re.form = ~ (1 | Participant)))

ggplot(mydata, aes(y=MLT, x=Time)) +
  facet_wrap(~ Participant) + 
  geom_point(aes(color=Participant), show.legend = FALSE) + 
  geom_line(aes(color=Participant), show.legend = FALSE) +
  geom_line(aes(y=int_model2_pred), linetype=2) +
  scale_y_continuous()
```

Figure 1. Predicted and actual MLT scores over time spent studying English

#### Random intercepts and random slopes

**Feel free to stop here in the tutorial! The rest of the information is for those who are particularly interested in this topic!** 

Above, we fit linear models with random intercepts, but fixed slopes. By doing so, we forced the model to assume that all individuals will follow the same slopes (i.e., increase/decrease the number of words per T-unit at the same rate over time). 

LMEs are quite flexible, however, and also allow us to fit more complicated models. For example, we can tell the model to adjust both the intercept (i.e., the starting MLT scores) and the slope (i.e., the rate and direction of change). More complicated models are often more difficult to interpret, but usually will account for more variance than simpler models. In general, we want to use models that optimize BOTH explanatory power and simplicity (more on this later).

Below, we will run a model with random intercepts AND random slopes, and then evaluate whether the more complicated model is reasonably better than the simpler one. We will almost always get a more accurate model when we add complexity, but the comlexity may not always be justified (e.g., it is a lot more difficult to explain a random intercepts and slopes model, and we only want to do so if it significantly increases our explanatory power). Note that we could treat Time either as a categorical variable or as a continuos variable. However, below we will only look at the continuous model.

```{r}
int_slope_model <- lmer(MLT~Time + (1 + Time | Participant), data=mydata)
summary(int_slope_model) #get model summary
```

As we see above, our results look almost identical to the previous model (int_model_2). The only difference is that we have statistics for one more random effect (Time) - our slope. Interestingly, once we added a random slope to the model, the variance in intercept vanished. In other words, the optimal model used the same intercept for all participants, but different slopes.

Below, we get by participant effects, which now include adjustments for slope AND intercept (though in this case the intercept is the same for all participants). We also see that our R^2m is almost identical, while our R^2c has increased slightly.

```{r}
ranef(int_slope_model)
r.squaredGLMM(int_slope_model)
```

When we plot our actual and predicted values, we now see that each participant has a slightly different slope. However, in this case, the differences are not extreme.

```{r}
library(dplyr)
mydata <- mydata %>% mutate(int_slope_model_pred = predict(int_slope_model,re.form = ~ (1 | Participant)))
ggplot(mydata, aes(y=MLT, x=Time)) +
  facet_wrap(~ Participant) + 
  geom_point(aes(color=Participant), show.legend = FALSE) + 
  geom_line(aes(color=Participant), show.legend = FALSE) +
  geom_line(aes(y=int_slope_model_pred), linetype=2) +
  scale_y_continuous()
```

To determine whether the more complicated model (random intercepts + slopes) is significantly better at modeling our data than the simpler one (random intercepts), we simply use the "anova()" function, which indicates that there are indeed no significant differences between our models (p = 0.648), so we might as well use the simpler one.

```{r}
anova(int_model2,int_slope_model)
```

